{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Using cached pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "Requirement already satisfied: py4j==0.10.9.2 in c:\\users\\omaim\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from pyspark) (0.10.9.2)\n",
      "Using legacy 'setup.py install' for pyspark, since package 'wheel' is not installed.\n",
      "Installing collected packages: pyspark\n",
      "    Running setup.py install for pyspark: started\n",
      "    Running setup.py install for pyspark: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'C:\\Users\\omaim\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\omaim\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4ldkjdbv\\\\pyspark_9fe30b5a991f4605b4d281583c930e0b\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\omaim\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4ldkjdbv\\\\pyspark_9fe30b5a991f4605b4d281583c930e0b\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\omaim\\AppData\\Local\\Temp\\pip-record-nuy9lixe\\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\\Users\\omaim\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Include\\pyspark'\n",
      "         cwd: C:\\Users\\omaim\\AppData\\Local\\Temp\\pip-install-4ldkjdbv\\pyspark_9fe30b5a991f4605b4d281583c930e0b\\\n",
      "    Complete output (826 lines):\n",
      "    C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.2544.0_x64__qbz5n2kfra8p0\\lib\\site-packages\\setuptools\\dist.py:717: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "      warnings.warn(\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build\\lib\n",
      "    creating build\\lib\\pyspark\n",
      "    copying pyspark\\accumulators.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\broadcast.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\conf.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\context.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\daemon.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\files.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\find_spark_home.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\install.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\java_gateway.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\join.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\profiler.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\rdd.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\rddsampler.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\resultiterable.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\serializers.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\shell.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\shuffle.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\statcounter.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\status.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\storagelevel.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\taskcontext.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\traceback_utils.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\util.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\version.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\worker.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\_globals.py -> build\\lib\\pyspark\n",
      "    copying pyspark\\__init__.py -> build\\lib\\pyspark\n",
      "    creating build\\lib\\pyspark\\cloudpickle\n",
      "    copying pyspark\\cloudpickle\\cloudpickle.py -> build\\lib\\pyspark\\cloudpickle\n",
      "    copying pyspark\\cloudpickle\\cloudpickle_fast.py -> build\\lib\\pyspark\\cloudpickle\n",
      "    copying pyspark\\cloudpickle\\compat.py -> build\\lib\\pyspark\\cloudpickle\n",
      "    copying pyspark\\cloudpickle\\__init__.py -> build\\lib\\pyspark\\cloudpickle\n",
      "    creating build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\classification.py -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\clustering.py -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\common.py -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\evaluation.py -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\feature.py -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\fpm.py -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\random.py -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\recommendation.py -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\regression.py -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\tree.py -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\util.py -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\__init__.py -> build\\lib\\pyspark\\mllib\n",
      "    creating build\\lib\\pyspark\\mllib\\linalg\n",
      "    copying pyspark\\mllib\\linalg\\distributed.py -> build\\lib\\pyspark\\mllib\\linalg\n",
      "    copying pyspark\\mllib\\linalg\\__init__.py -> build\\lib\\pyspark\\mllib\\linalg\n",
      "    creating build\\lib\\pyspark\\mllib\\stat\n",
      "    copying pyspark\\mllib\\stat\\distribution.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "    copying pyspark\\mllib\\stat\\KernelDensity.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "    copying pyspark\\mllib\\stat\\test.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "    copying pyspark\\mllib\\stat\\_statistics.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "    copying pyspark\\mllib\\stat\\__init__.py -> build\\lib\\pyspark\\mllib\\stat\n",
      "    creating build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\base.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\classification.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\clustering.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\common.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\evaluation.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\feature.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\fpm.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\functions.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\image.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\pipeline.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\recommendation.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\regression.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\stat.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\tree.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\tuning.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\util.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\wrapper.py -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\__init__.py -> build\\lib\\pyspark\\ml\n",
      "    creating build\\lib\\pyspark\\ml\\linalg\n",
      "    copying pyspark\\ml\\linalg\\__init__.py -> build\\lib\\pyspark\\ml\\linalg\n",
      "    creating build\\lib\\pyspark\\ml\\param\n",
      "    copying pyspark\\ml\\param\\shared.py -> build\\lib\\pyspark\\ml\\param\n",
      "    copying pyspark\\ml\\param\\_shared_params_code_gen.py -> build\\lib\\pyspark\\ml\\param\n",
      "    copying pyspark\\ml\\param\\__init__.py -> build\\lib\\pyspark\\ml\\param\n",
      "    creating build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\catalog.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\column.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\conf.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\context.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\dataframe.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\functions.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\group.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\readwriter.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\session.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\streaming.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\types.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\udf.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\utils.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\window.py -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\__init__.py -> build\\lib\\pyspark\\sql\n",
      "    creating build\\lib\\pyspark\\sql\\avro\n",
      "    copying pyspark\\sql\\avro\\functions.py -> build\\lib\\pyspark\\sql\\avro\n",
      "    copying pyspark\\sql\\avro\\__init__.py -> build\\lib\\pyspark\\sql\\avro\n",
      "    creating build\\lib\\pyspark\\sql\\pandas\n",
      "    copying pyspark\\sql\\pandas\\conversion.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "    copying pyspark\\sql\\pandas\\functions.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "    copying pyspark\\sql\\pandas\\group_ops.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "    copying pyspark\\sql\\pandas\\map_ops.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "    copying pyspark\\sql\\pandas\\serializers.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "    copying pyspark\\sql\\pandas\\typehints.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "    copying pyspark\\sql\\pandas\\types.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "    copying pyspark\\sql\\pandas\\utils.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "    copying pyspark\\sql\\pandas\\__init__.py -> build\\lib\\pyspark\\sql\\pandas\n",
      "    creating build\\lib\\pyspark\\streaming\n",
      "    copying pyspark\\streaming\\context.py -> build\\lib\\pyspark\\streaming\n",
      "    copying pyspark\\streaming\\dstream.py -> build\\lib\\pyspark\\streaming\n",
      "    copying pyspark\\streaming\\kinesis.py -> build\\lib\\pyspark\\streaming\n",
      "    copying pyspark\\streaming\\listener.py -> build\\lib\\pyspark\\streaming\n",
      "    copying pyspark\\streaming\\util.py -> build\\lib\\pyspark\\streaming\n",
      "    copying pyspark\\streaming\\__init__.py -> build\\lib\\pyspark\\streaming\n",
      "    package init file 'deps\\bin\\__init__.py' not found (or not a regular file)\n",
      "    package init file 'deps\\sbin\\__init__.py' not found (or not a regular file)\n",
      "    package init file 'deps\\jars\\__init__.py' not found (or not a regular file)\n",
      "    creating build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\accessors.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\base.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\categorical.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\config.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\datetimes.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\exceptions.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\extensions.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\frame.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\generic.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\groupby.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\indexing.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\internal.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\ml.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\mlflow.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\namespace.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\numpy_compat.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\series.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\sql_processor.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\strings.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\utils.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\window.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\_typing.py -> build\\lib\\pyspark\\pandas\n",
      "    copying pyspark\\pandas\\__init__.py -> build\\lib\\pyspark\\pandas\n",
      "    creating build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    copying pyspark\\pandas\\data_type_ops\\base.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    copying pyspark\\pandas\\data_type_ops\\binary_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    copying pyspark\\pandas\\data_type_ops\\boolean_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    copying pyspark\\pandas\\data_type_ops\\categorical_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    copying pyspark\\pandas\\data_type_ops\\complex_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    copying pyspark\\pandas\\data_type_ops\\datetime_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    copying pyspark\\pandas\\data_type_ops\\date_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    copying pyspark\\pandas\\data_type_ops\\null_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    copying pyspark\\pandas\\data_type_ops\\num_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    copying pyspark\\pandas\\data_type_ops\\string_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    copying pyspark\\pandas\\data_type_ops\\udt_ops.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    copying pyspark\\pandas\\data_type_ops\\__init__.py -> build\\lib\\pyspark\\pandas\\data_type_ops\n",
      "    creating build\\lib\\pyspark\\pandas\\indexes\n",
      "    copying pyspark\\pandas\\indexes\\base.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "    copying pyspark\\pandas\\indexes\\category.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "    copying pyspark\\pandas\\indexes\\datetimes.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "    copying pyspark\\pandas\\indexes\\multi.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "    copying pyspark\\pandas\\indexes\\numeric.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "    copying pyspark\\pandas\\indexes\\__init__.py -> build\\lib\\pyspark\\pandas\\indexes\n",
      "    creating build\\lib\\pyspark\\pandas\\missing\n",
      "    copying pyspark\\pandas\\missing\\common.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "    copying pyspark\\pandas\\missing\\frame.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "    copying pyspark\\pandas\\missing\\groupby.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "    copying pyspark\\pandas\\missing\\indexes.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "    copying pyspark\\pandas\\missing\\series.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "    copying pyspark\\pandas\\missing\\window.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "    copying pyspark\\pandas\\missing\\__init__.py -> build\\lib\\pyspark\\pandas\\missing\n",
      "    creating build\\lib\\pyspark\\pandas\\plot\n",
      "    copying pyspark\\pandas\\plot\\core.py -> build\\lib\\pyspark\\pandas\\plot\n",
      "    copying pyspark\\pandas\\plot\\matplotlib.py -> build\\lib\\pyspark\\pandas\\plot\n",
      "    copying pyspark\\pandas\\plot\\plotly.py -> build\\lib\\pyspark\\pandas\\plot\n",
      "    copying pyspark\\pandas\\plot\\__init__.py -> build\\lib\\pyspark\\pandas\\plot\n",
      "    creating build\\lib\\pyspark\\pandas\\spark\n",
      "    copying pyspark\\pandas\\spark\\accessors.py -> build\\lib\\pyspark\\pandas\\spark\n",
      "    copying pyspark\\pandas\\spark\\functions.py -> build\\lib\\pyspark\\pandas\\spark\n",
      "    copying pyspark\\pandas\\spark\\utils.py -> build\\lib\\pyspark\\pandas\\spark\n",
      "    copying pyspark\\pandas\\spark\\__init__.py -> build\\lib\\pyspark\\pandas\\spark\n",
      "    creating build\\lib\\pyspark\\pandas\\typedef\n",
      "    copying pyspark\\pandas\\typedef\\string_typehints.py -> build\\lib\\pyspark\\pandas\\typedef\n",
      "    copying pyspark\\pandas\\typedef\\typehints.py -> build\\lib\\pyspark\\pandas\\typedef\n",
      "    copying pyspark\\pandas\\typedef\\__init__.py -> build\\lib\\pyspark\\pandas\\typedef\n",
      "    creating build\\lib\\pyspark\\pandas\\usage_logging\n",
      "    copying pyspark\\pandas\\usage_logging\\usage_logger.py -> build\\lib\\pyspark\\pandas\\usage_logging\n",
      "    copying pyspark\\pandas\\usage_logging\\__init__.py -> build\\lib\\pyspark\\pandas\\usage_logging\n",
      "    package init file 'pyspark\\python\\pyspark\\__init__.py' not found (or not a regular file)\n",
      "    creating build\\lib\\pyspark\\python\n",
      "    creating build\\lib\\pyspark\\python\\pyspark\n",
      "    copying pyspark\\python\\pyspark\\shell.py -> build\\lib\\pyspark\\python\\pyspark\n",
      "    package init file 'lib\\__init__.py' not found (or not a regular file)\n",
      "    package init file 'deps\\data\\__init__.py' not found (or not a regular file)\n",
      "    package init file 'deps\\licenses\\__init__.py' not found (or not a regular file)\n",
      "    creating build\\lib\\pyspark\\resource\n",
      "    copying pyspark\\resource\\information.py -> build\\lib\\pyspark\\resource\n",
      "    copying pyspark\\resource\\profile.py -> build\\lib\\pyspark\\resource\n",
      "    copying pyspark\\resource\\requests.py -> build\\lib\\pyspark\\resource\n",
      "    copying pyspark\\resource\\__init__.py -> build\\lib\\pyspark\\resource\n",
      "    package init file 'deps\\examples\\__init__.py' not found (or not a regular file)\n",
      "    creating build\\lib\\pyspark\\examples\n",
      "    creating build\\lib\\pyspark\\examples\\src\n",
      "    creating build\\lib\\pyspark\\examples\\src\\main\n",
      "    creating build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "    copying deps\\examples\\als.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "    copying deps\\examples\\avro_inputformat.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "    copying deps\\examples\\kmeans.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "    copying deps\\examples\\logistic_regression.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "    copying deps\\examples\\pagerank.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "    copying deps\\examples\\parquet_inputformat.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "    copying deps\\examples\\pi.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "    copying deps\\examples\\sort.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "    copying deps\\examples\\status_api_demo.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "    copying deps\\examples\\transitive_closure.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "    copying deps\\examples\\wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\n",
      "    running egg_info\n",
      "    writing pyspark.egg-info\\PKG-INFO\n",
      "    writing dependency_links to pyspark.egg-info\\dependency_links.txt\n",
      "    writing requirements to pyspark.egg-info\\requires.txt\n",
      "    writing top-level names to pyspark.egg-info\\top_level.txt\n",
      "    reading manifest file 'pyspark.egg-info\\SOURCES.txt'\n",
      "    reading manifest template 'MANIFEST.in'\n",
      "    warning: no previously-included files matching '*.py[cod]' found anywhere in distribution\n",
      "    warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
      "    warning: no previously-included files matching '.DS_Store' found anywhere in distribution\n",
      "    writing manifest file 'pyspark.egg-info\\SOURCES.txt'\n",
      "    copying pyspark\\__init__.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\_typing.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\accumulators.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\broadcast.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\conf.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\context.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\files.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\profiler.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\py.typed -> build\\lib\\pyspark\n",
      "    copying pyspark\\rdd.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\resultiterable.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\statcounter.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\status.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\storagelevel.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\taskcontext.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\util.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\version.pyi -> build\\lib\\pyspark\n",
      "    copying pyspark\\mllib\\_typing.pyi -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\classification.pyi -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\clustering.pyi -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\common.pyi -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\evaluation.pyi -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\feature.pyi -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\fpm.pyi -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\random.pyi -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\recommendation.pyi -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\regression.pyi -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\tree.pyi -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\util.pyi -> build\\lib\\pyspark\\mllib\n",
      "    copying pyspark\\mllib\\linalg\\__init__.pyi -> build\\lib\\pyspark\\mllib\\linalg\n",
      "    copying pyspark\\mllib\\linalg\\distributed.pyi -> build\\lib\\pyspark\\mllib\\linalg\n",
      "    copying pyspark\\mllib\\stat\\KernelDensity.pyi -> build\\lib\\pyspark\\mllib\\stat\n",
      "    copying pyspark\\mllib\\stat\\__init__.pyi -> build\\lib\\pyspark\\mllib\\stat\n",
      "    copying pyspark\\mllib\\stat\\_statistics.pyi -> build\\lib\\pyspark\\mllib\\stat\n",
      "    copying pyspark\\mllib\\stat\\distribution.pyi -> build\\lib\\pyspark\\mllib\\stat\n",
      "    copying pyspark\\mllib\\stat\\test.pyi -> build\\lib\\pyspark\\mllib\\stat\n",
      "    copying pyspark\\ml\\_typing.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\base.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\classification.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\clustering.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\common.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\evaluation.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\feature.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\fpm.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\functions.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\image.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\pipeline.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\recommendation.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\regression.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\stat.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\tree.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\tuning.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\util.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\wrapper.pyi -> build\\lib\\pyspark\\ml\n",
      "    copying pyspark\\ml\\linalg\\__init__.pyi -> build\\lib\\pyspark\\ml\\linalg\n",
      "    copying pyspark\\ml\\param\\__init__.pyi -> build\\lib\\pyspark\\ml\\param\n",
      "    copying pyspark\\ml\\param\\_shared_params_code_gen.pyi -> build\\lib\\pyspark\\ml\\param\n",
      "    copying pyspark\\ml\\param\\shared.pyi -> build\\lib\\pyspark\\ml\\param\n",
      "    copying pyspark\\sql\\__init__.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\_typing.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\catalog.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\column.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\conf.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\context.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\dataframe.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\functions.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\group.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\readwriter.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\session.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\streaming.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\types.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\udf.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\window.pyi -> build\\lib\\pyspark\\sql\n",
      "    copying pyspark\\sql\\avro\\functions.pyi -> build\\lib\\pyspark\\sql\\avro\n",
      "    copying pyspark\\sql\\pandas\\conversion.pyi -> build\\lib\\pyspark\\sql\\pandas\n",
      "    copying pyspark\\sql\\pandas\\functions.pyi -> build\\lib\\pyspark\\sql\\pandas\n",
      "    copying pyspark\\sql\\pandas\\group_ops.pyi -> build\\lib\\pyspark\\sql\\pandas\n",
      "    copying pyspark\\sql\\pandas\\map_ops.pyi -> build\\lib\\pyspark\\sql\\pandas\n",
      "    creating build\\lib\\pyspark\\sql\\pandas\\_typing\n",
      "    copying pyspark\\sql\\pandas\\_typing\\__init__.pyi -> build\\lib\\pyspark\\sql\\pandas\\_typing\n",
      "    creating build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "    copying pyspark\\sql\\pandas\\_typing\\protocols\\__init__.pyi -> build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "    copying pyspark\\sql\\pandas\\_typing\\protocols\\frame.pyi -> build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "    copying pyspark\\sql\\pandas\\_typing\\protocols\\series.pyi -> build\\lib\\pyspark\\sql\\pandas\\_typing\\protocols\n",
      "    copying pyspark\\streaming\\context.pyi -> build\\lib\\pyspark\\streaming\n",
      "    copying pyspark\\streaming\\dstream.pyi -> build\\lib\\pyspark\\streaming\n",
      "    copying pyspark\\streaming\\kinesis.pyi -> build\\lib\\pyspark\\streaming\n",
      "    copying pyspark\\streaming\\listener.pyi -> build\\lib\\pyspark\\streaming\n",
      "    creating build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\beeline -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\beeline.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\docker-image-tool.sh -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\find-spark-home -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\find-spark-home.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\load-spark-env.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\load-spark-env.sh -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\pyspark -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\pyspark.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\pyspark2.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\run-example -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\run-example.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\spark-class -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\spark-class.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\spark-class2.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\spark-shell -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\spark-shell.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\spark-shell2.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\spark-sql -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\spark-sql.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\spark-sql2.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\spark-submit -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\spark-submit.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\spark-submit2.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\sparkR -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\sparkR.cmd -> build\\lib\\pyspark\\bin\n",
      "    copying deps\\bin\\sparkR2.cmd -> build\\lib\\pyspark\\bin\n",
      "    creating build\\lib\\pyspark\\sbin\n",
      "    copying deps\\sbin\\spark-config.sh -> build\\lib\\pyspark\\sbin\n",
      "    copying deps\\sbin\\spark-daemon.sh -> build\\lib\\pyspark\\sbin\n",
      "    copying deps\\sbin\\start-history-server.sh -> build\\lib\\pyspark\\sbin\n",
      "    copying deps\\sbin\\stop-history-server.sh -> build\\lib\\pyspark\\sbin\n",
      "    creating build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\HikariCP-2.5.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\JLargeArrays-1.5.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\JTransforms-3.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\RoaringBitmap-0.9.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\ST4-4.0.4.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\activation-1.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\aircompressor-0.21.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\algebra_2.12-2.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\annotations-17.0.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\antlr-runtime-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\antlr4-runtime-4.8.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\aopalliance-repackaged-2.6.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\arpack-2.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\arpack_combined_all-0.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\arrow-format-2.0.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\arrow-memory-core-2.0.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\arrow-memory-netty-2.0.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\arrow-vector-2.0.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\audience-annotations-0.5.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\automaton-1.11-8.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\avro-1.10.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\avro-ipc-1.10.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\avro-mapred-1.10.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\blas-2.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\bonecp-0.8.0.RELEASE.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\breeze-macros_2.12-1.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\breeze_2.12-1.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\cats-kernel_2.12-2.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\chill-java-0.10.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\chill_2.12-0.10.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-cli-1.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-codec-1.15.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-collections-3.2.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-compiler-3.0.16.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-compress-1.21.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-crypto-1.1.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-dbcp-1.4.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-io-2.8.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-lang-2.6.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-lang3-3.12.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-logging-1.1.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-math3-3.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-net-3.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-pool-1.5.4.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\commons-text-1.6.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\compress-lzf-1.0.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\core-1.1.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\curator-client-2.13.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\curator-framework-2.13.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\curator-recipes-2.13.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\datanucleus-api-jdo-4.2.4.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\datanucleus-core-4.1.17.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\datanucleus-rdbms-4.1.19.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\derby-10.14.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\flatbuffers-java-1.9.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\generex-1.0.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\gson-2.2.4.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\guava-14.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hadoop-client-api-3.3.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hadoop-client-runtime-3.3.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hadoop-shaded-guava-1.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hadoop-yarn-server-web-proxy-3.3.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-beeline-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-cli-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-common-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-exec-2.3.9-core.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-jdbc-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-llap-common-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-metastore-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-serde-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-service-rpc-3.1.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-shims-0.23-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-shims-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-shims-common-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-shims-scheduler-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-storage-api-2.7.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hive-vector-code-gen-2.3.9.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hk2-api-2.6.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hk2-locator-2.6.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\hk2-utils-2.6.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\htrace-core4-4.1.0-incubating.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\httpclient-4.5.13.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\httpcore-4.4.14.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\istack-commons-runtime-3.0.8.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\ivy-2.5.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jackson-annotations-2.12.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jackson-core-2.12.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jackson-core-asl-1.9.13.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jackson-databind-2.12.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jackson-dataformat-yaml-2.12.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jackson-datatype-jsr310-2.11.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jackson-mapper-asl-1.9.13.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jackson-module-scala_2.12-2.12.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jakarta.annotation-api-1.3.5.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jakarta.inject-2.6.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jakarta.servlet-api-4.0.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jakarta.validation-api-2.0.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jakarta.ws.rs-api-2.1.6.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jakarta.xml.bind-api-2.3.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\janino-3.0.16.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\javassist-3.25.0-GA.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\javax.jdo-3.2.0-m3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\javolution-5.5.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jaxb-api-2.2.11.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jaxb-runtime-2.3.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jcl-over-slf4j-1.7.30.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jdo-api-3.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jersey-client-2.34.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jersey-common-2.34.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jersey-container-servlet-2.34.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jersey-container-servlet-core-2.34.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jersey-hk2-2.34.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jersey-server-2.34.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jline-2.14.6.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\joda-time-2.10.10.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jodd-core-3.5.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jpam-1.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\json-1.8.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\json4s-ast_2.12-3.7.0-M11.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\json4s-core_2.12-3.7.0-M11.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\json4s-jackson_2.12-3.7.0-M11.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\json4s-scalap_2.12-3.7.0-M11.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jsr305-3.0.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jta-1.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\jul-to-slf4j-1.7.30.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kryo-shaded-4.0.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-client-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-admissionregistration-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-apiextensions-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-apps-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-autoscaling-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-batch-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-certificates-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-common-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-coordination-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-core-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-discovery-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-events-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-extensions-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-flowcontrol-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-metrics-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-networking-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-node-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-policy-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-rbac-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-scheduling-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\kubernetes-model-storageclass-5.4.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\lapack-2.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\leveldbjni-all-1.8.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\libfb303-0.9.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\libthrift-0.12.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\log4j-1.2.17.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\logging-interceptor-3.12.12.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\lz4-java-1.7.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\macro-compat_2.12-1.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\mesos-1.4.0-shaded-protobuf.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\metrics-core-4.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\metrics-graphite-4.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\metrics-jmx-4.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\metrics-json-4.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\metrics-jvm-4.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\minlog-1.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\netty-all-4.1.68.Final.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\objenesis-2.6.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\okhttp-3.12.12.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\okio-1.14.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\opencsv-2.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\orc-core-1.6.11.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\orc-mapreduce-1.6.11.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\orc-shims-1.6.11.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\oro-2.0.8.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\osgi-resource-locator-1.0.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\paranamer-2.8.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\parquet-column-1.12.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\parquet-common-1.12.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\parquet-encoding-1.12.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\parquet-format-structures-1.12.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\parquet-hadoop-1.12.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\parquet-jackson-1.12.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\protobuf-java-2.5.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\py4j-0.10.9.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\pyrolite-4.30.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\rocksdbjni-6.20.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\scala-collection-compat_2.12-2.1.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\scala-compiler-2.12.15.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\scala-library-2.12.15.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\scala-parser-combinators_2.12-1.1.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\scala-reflect-2.12.15.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\scala-xml_2.12-1.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\shapeless_2.12-2.3.3.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\shims-0.9.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\slf4j-api-1.7.30.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\slf4j-log4j12-1.7.30.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\snakeyaml-1.27.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\snappy-java-1.1.8.4.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-catalyst_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-core_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-graphx_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-hive-thriftserver_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-hive_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-kubernetes_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-kvstore_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-launcher_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-mesos_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-mllib-local_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-mllib_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-network-common_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-network-shuffle_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-repl_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-sketch_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-sql_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-streaming_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-tags_2.12-3.2.0-tests.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-tags_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-unsafe_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spark-yarn_2.12-3.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spire-macros_2.12-0.17.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spire-platform_2.12-0.17.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spire-util_2.12-0.17.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\spire_2.12-0.17.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\stax-api-1.0.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\stream-2.9.6.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\super-csv-2.2.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\threeten-extra-1.5.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\tink-1.6.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\transaction-api-1.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\univocity-parsers-2.9.1.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\velocity-1.5.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\xbean-asm9-shaded-4.20.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\xz-1.8.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\zjsonpatch-0.3.0.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\zookeeper-3.6.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\zookeeper-jute-3.6.2.jar -> build\\lib\\pyspark\\jars\n",
      "    copying deps\\jars\\zstd-jni-1.5.0-4.jar -> build\\lib\\pyspark\\jars\n",
      "    creating build\\lib\\pyspark\\python\\lib\n",
      "    copying lib\\py4j-0.10.9.2-src.zip -> build\\lib\\pyspark\\python\\lib\n",
      "    copying lib\\pyspark.zip -> build\\lib\\pyspark\\python\\lib\n",
      "    creating build\\lib\\pyspark\\data\n",
      "    creating build\\lib\\pyspark\\data\\graphx\n",
      "    copying deps\\data\\graphx\\followers.txt -> build\\lib\\pyspark\\data\\graphx\n",
      "    copying deps\\data\\graphx\\users.txt -> build\\lib\\pyspark\\data\\graphx\n",
      "    creating build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\gmm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\iris_libsvm.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\kmeans_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\pagerank_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\pic_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\sample_binary_classification_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\sample_fpgrowth.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\sample_isotonic_regression_libsvm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\sample_kmeans_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\sample_lda_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\sample_lda_libsvm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\sample_libsvm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\sample_linear_regression_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\sample_movielens_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\sample_multiclass_classification_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\sample_svm_data.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    copying deps\\data\\mllib\\streaming_kmeans_data_test.txt -> build\\lib\\pyspark\\data\\mllib\n",
      "    creating build\\lib\\pyspark\\data\\mllib\\als\n",
      "    copying deps\\data\\mllib\\als\\sample_movielens_ratings.txt -> build\\lib\\pyspark\\data\\mllib\\als\n",
      "    copying deps\\data\\mllib\\als\\test.data -> build\\lib\\pyspark\\data\\mllib\\als\n",
      "    creating build\\lib\\pyspark\\data\\mllib\\images\n",
      "    copying deps\\data\\mllib\\images\\license.txt -> build\\lib\\pyspark\\data\\mllib\\images\n",
      "    creating build\\lib\\pyspark\\data\\mllib\\images\\origin\n",
      "    copying deps\\data\\mllib\\images\\origin\\license.txt -> build\\lib\\pyspark\\data\\mllib\\images\\origin\n",
      "    creating build\\lib\\pyspark\\data\\mllib\\images\\origin\\kittens\n",
      "    copying deps\\data\\mllib\\images\\origin\\kittens\\not-image.txt -> build\\lib\\pyspark\\data\\mllib\\images\\origin\\kittens\n",
      "    creating build\\lib\\pyspark\\data\\mllib\\images\\partitioned\n",
      "    creating build\\lib\\pyspark\\data\\mllib\\images\\partitioned\\cls=kittens\n",
      "    creating build\\lib\\pyspark\\data\\mllib\\images\\partitioned\\cls=kittens\\date=2018-01\n",
      "    copying deps\\data\\mllib\\images\\partitioned\\cls=kittens\\date=2018-01\\not-image.txt -> build\\lib\\pyspark\\data\\mllib\\images\\partitioned\\cls=kittens\\date=2018-01\n",
      "    creating build\\lib\\pyspark\\data\\mllib\\ridge-data\n",
      "    copying deps\\data\\mllib\\ridge-data\\lpsa.data -> build\\lib\\pyspark\\data\\mllib\\ridge-data\n",
      "    creating build\\lib\\pyspark\\data\\streaming\n",
      "    copying deps\\data\\streaming\\AFINN-111.txt -> build\\lib\\pyspark\\data\\streaming\n",
      "    creating build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-AnchorJS.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-CC0.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-bootstrap.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-cloudpickle.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-copybutton.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-d3.min.js.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-dagre-d3.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-datatables.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-graphlib-dot.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-join.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-jquery.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-json-formatter.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-matchMedia-polyfill.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-modernizr.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-mustache.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-py4j.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-respond.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-sbt-launch-lib.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-sorttable.js.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying deps\\licenses\\LICENSE-vis-timeline.txt -> build\\lib\\pyspark\\licenses\n",
      "    copying pyspark\\resource\\information.pyi -> build\\lib\\pyspark\\resource\n",
      "    copying pyspark\\resource\\profile.pyi -> build\\lib\\pyspark\\resource\n",
      "    copying pyspark\\resource\\requests.pyi -> build\\lib\\pyspark\\resource\n",
      "    creating build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\aft_survival_regression.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\als_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\binarizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\bisecting_k_means_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\bucketed_random_projection_lsh_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\bucketizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\chi_square_test_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\chisq_selector_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\correlation_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\count_vectorizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\cross_validator.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\dataframe_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\dct_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\decision_tree_classification_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\decision_tree_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\elementwise_product_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\estimator_transformer_param_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\feature_hasher_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\fm_classifier_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\fm_regressor_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\fpgrowth_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\gaussian_mixture_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\generalized_linear_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\gradient_boosted_tree_classifier_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\gradient_boosted_tree_regressor_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\imputer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\index_to_string_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\interaction_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\isotonic_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\kmeans_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\lda_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\linear_regression_with_elastic_net.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\linearsvc.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\logistic_regression_summary_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\logistic_regression_with_elastic_net.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\max_abs_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\min_hash_lsh_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\min_max_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\multiclass_logistic_regression_with_elastic_net.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\multilayer_perceptron_classification.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\n_gram_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\naive_bayes_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\normalizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\one_vs_rest_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\onehot_encoder_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\pca_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\pipeline_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\polynomial_expansion_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\power_iteration_clustering_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\prefixspan_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\quantile_discretizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\random_forest_classifier_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\random_forest_regressor_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\rformula_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\robust_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\sql_transformer.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\standard_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\stopwords_remover_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\string_indexer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\summarizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\tf_idf_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\tokenizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\train_validation_split.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\univariate_feature_selector_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\variance_threshold_selector_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\vector_assembler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\vector_indexer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\vector_size_hint_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\vector_slicer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    copying deps\\examples\\ml\\word2vec_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\ml\n",
      "    creating build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\binary_classification_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\bisecting_k_means_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\correlations.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\correlations_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\decision_tree_classification_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\decision_tree_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\elementwise_product_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\fpgrowth_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\gaussian_mixture_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\gaussian_mixture_model.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\gradient_boosting_classification_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\gradient_boosting_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\hypothesis_testing_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\hypothesis_testing_kolmogorov_smirnov_test_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\isotonic_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\k_means_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\kernel_density_estimation_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\kmeans.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\latent_dirichlet_allocation_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\linear_regression_with_sgd_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\logistic_regression.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\logistic_regression_with_lbfgs_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\multi_class_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\multi_label_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\naive_bayes_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\normalizer_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\pca_rowmatrix_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\power_iteration_clustering_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\random_forest_classification_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\random_forest_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\random_rdd_generation.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\ranking_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\recommendation_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\regression_metrics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\sampled_rdds.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\standard_scaler_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\stratified_sampling_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\streaming_k_means_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\streaming_linear_regression_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\summary_statistics_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\svd_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\svm_with_sgd_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\tf_idf_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\word2vec.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    copying deps\\examples\\mllib\\word2vec_example.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\mllib\n",
      "    creating build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "    copying deps\\examples\\sql\\arrow.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "    copying deps\\examples\\sql\\basic.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "    copying deps\\examples\\sql\\datasource.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "    copying deps\\examples\\sql\\hive.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\n",
      "    creating build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "    copying deps\\examples\\sql\\streaming\\structured_kafka_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "    copying deps\\examples\\sql\\streaming\\structured_network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "    copying deps\\examples\\sql\\streaming\\structured_network_wordcount_windowed.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "    copying deps\\examples\\sql\\streaming\\structured_sessionization.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\sql\\streaming\n",
      "    creating build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "    copying deps\\examples\\streaming\\hdfs_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "    copying deps\\examples\\streaming\\network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "    copying deps\\examples\\streaming\\network_wordjoinsentiments.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "    copying deps\\examples\\streaming\\queue_stream.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "    copying deps\\examples\\streaming\\recoverable_network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "    copying deps\\examples\\streaming\\sql_network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "    copying deps\\examples\\streaming\\stateful_network_wordcount.py -> build\\lib\\pyspark\\examples\\src\\main\\python\\streaming\n",
      "    running build_scripts\n",
      "    creating build\\scripts-3.9\n",
      "    copying deps\\bin\\beeline -> build\\scripts-3.9\n",
      "    copying deps\\bin\\beeline.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\docker-image-tool.sh -> build\\scripts-3.9\n",
      "    copying deps\\bin\\find-spark-home -> build\\scripts-3.9\n",
      "    copying deps\\bin\\find-spark-home.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\load-spark-env.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\load-spark-env.sh -> build\\scripts-3.9\n",
      "    copying deps\\bin\\pyspark -> build\\scripts-3.9\n",
      "    copying deps\\bin\\pyspark.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\pyspark2.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\run-example -> build\\scripts-3.9\n",
      "    copying deps\\bin\\run-example.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\spark-class -> build\\scripts-3.9\n",
      "    copying deps\\bin\\spark-class.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\spark-class2.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\spark-shell -> build\\scripts-3.9\n",
      "    copying deps\\bin\\spark-shell.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\spark-shell2.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\spark-sql -> build\\scripts-3.9\n",
      "    copying deps\\bin\\spark-sql.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\spark-sql2.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\spark-submit -> build\\scripts-3.9\n",
      "    copying deps\\bin\\spark-submit.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\spark-submit2.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\sparkR -> build\\scripts-3.9\n",
      "    copying deps\\bin\\sparkR.cmd -> build\\scripts-3.9\n",
      "    copying deps\\bin\\sparkR2.cmd -> build\\scripts-3.9\n",
      "    copying and adjusting pyspark\\find_spark_home.py -> build\\scripts-3.9\n",
      "    running install_lib\n",
      "    copying build\\lib\\pyspark\\python\\pyspark\\shell.py -> C:\\Users\\omaim\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\python\\pyspark\n",
      "    byte-compiling C:\\Users\\omaim\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\examples\\src\\main\\python\\ml\\multiclass_logistic_regression_with_elastic_net.py to multiclass_logistic_regression_with_elastic_net.cpython-39.pyc\n",
      "    error: [Errno 2] No such file or directory: 'C:\\\\Users\\\\omaim\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python39\\\\site-packages\\\\pyspark\\\\examples\\\\src\\\\main\\\\python\\\\ml\\\\__pycache__\\\\multiclass_logistic_regression_with_elastic_net.cpython-39.pyc.3021221250288'\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'C:\\Users\\omaim\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\omaim\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4ldkjdbv\\\\pyspark_9fe30b5a991f4605b4d281583c930e0b\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\omaim\\\\AppData\\\\Local\\\\Temp\\\\pip-install-4ldkjdbv\\\\pyspark_9fe30b5a991f4605b4d281583c930e0b\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\omaim\\AppData\\Local\\Temp\\pip-record-nuy9lixe\\install-record.txt' --single-version-externally-managed --user --prefix= --compile --install-headers 'C:\\Users\\omaim\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\Include\\pyspark' Check the logs for full command output.\n",
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\omaim\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\omaim\\\\AppData\\\\Local\\\\Temp\\\\spark-eea789ae-a00a-4159-bf96-8a593e61e7b3\\\\pyspark-aa7ca6d1-7e65-427b-a2c3-f52ef6f93445\\\\tmp87g3evln'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18024/1508461869.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m df = spark.createDataFrame([\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'string1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'string2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[0;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[1;32m--> 675\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[1;31m# convert python objects to sql data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoInternal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mparallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonParallelizeServer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_serialize_to_jvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreateRDDServer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_serialize_to_jvm\u001b[1;34m(self, data, serializer, reader_func, createRDDServer)\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[1;31m# without encryption, we serialize to a file, and we read the file in java and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m             \u001b[1;31m# parallelize from there.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m             \u001b[0mtempFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.2544.0_x64__qbz5n2kfra8p0\\lib\\tempfile.py\u001b[0m in \u001b[0;36mNamedTemporaryFile\u001b[1;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mO_TEMPORARY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_mkstemp_inner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         file = _io.open(fd, mode, buffering=buffering,\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.2544.0_x64__qbz5n2kfra8p0\\lib\\tempfile.py\u001b[0m in \u001b[0;36m_mkstemp_inner\u001b[1;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tempfile.mkstemp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             \u001b[0mfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0o600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[1;32mcontinue\u001b[0m    \u001b[1;31m# try again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\omaim\\\\AppData\\\\Local\\\\Temp\\\\spark-eea789ae-a00a-4159-bf96-8a593e61e7b3\\\\pyspark-aa7ca6d1-7e65-427b-a2c3-f52ef6f93445\\\\tmp87g3evln'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.persist()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------+\n",
      "|summary|                 a|                 b|      c|\n",
      "+-------+------------------+------------------+-------+\n",
      "|  count|                 3|                 3|      3|\n",
      "|   mean|2.3333333333333335|3.3333333333333335|   null|\n",
      "| stddev|1.5275252316519468|1.5275252316519468|   null|\n",
      "|    min|                 1|               2.0|string1|\n",
      "|    max|                 4|               5.0|string3|\n",
      "+-------+------------------+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"a\",\"b\", \"c\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "<tr><td>4</td><td>5.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n",
       " Row(a=4, b=5.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\omaim\\\\AppData\\\\Local\\\\Temp\\\\spark-eea789ae-a00a-4159-bf96-8a593e61e7b3\\\\pyspark-aa7ca6d1-7e65-427b-a2c3-f52ef6f93445\\\\tmp1ulidd47'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18024/730558501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m df1 = spark.createDataFrame(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20000101\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m20000101\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m20000102\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m20000102\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     ('time', 'id', 'v1'))\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m df2 = spark.createDataFrame(\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[0;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[1;32m--> 675\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    701\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[1;31m# convert python objects to sql data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoInternal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mparallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonParallelizeServer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_serialize_to_jvm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreader_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreateRDDServer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_serialize_to_jvm\u001b[1;34m(self, data, serializer, reader_func, createRDDServer)\u001b[0m\n\u001b[0;32m    606\u001b[0m             \u001b[1;31m# without encryption, we serialize to a file, and we read the file in java and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m             \u001b[1;31m# parallelize from there.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m             \u001b[0mtempFile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNamedTemporaryFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.2544.0_x64__qbz5n2kfra8p0\\lib\\tempfile.py\u001b[0m in \u001b[0;36mNamedTemporaryFile\u001b[1;34m(mode, buffering, encoding, newline, suffix, prefix, dir, delete, errors)\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mO_TEMPORARY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_mkstemp_inner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         file = _io.open(fd, mode, buffering=buffering,\n",
      "\u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.2544.0_x64__qbz5n2kfra8p0\\lib\\tempfile.py\u001b[0m in \u001b[0;36m_mkstemp_inner\u001b[1;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tempfile.mkstemp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             \u001b[0mfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0o600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m             \u001b[1;32mcontinue\u001b[0m    \u001b[1;31m# try again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\omaim\\\\AppData\\\\Local\\\\Temp\\\\spark-eea789ae-a00a-4159-bf96-8a593e61e7b3\\\\pyspark-aa7ca6d1-7e65-427b-a2c3-f52ef6f93445\\\\tmp1ulidd47'"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    ('time', 'id', 'v1'))\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n",
    "    ('time', 'id', 'v2'))\n",
    "\n",
    "def asof_join(l, r):\n",
    "    return pd.merge_asof(l, r, on='time', by='id')\n",
    "\n",
    "df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(\n",
    "    asof_join, schema='time int, id int, v1 double, v2 string').show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e949ccebb1c5a1aea0eed53c0616c5475e87a9ddaac920f369ddf5732a894eff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
